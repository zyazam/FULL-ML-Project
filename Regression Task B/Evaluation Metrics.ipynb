{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "name": "TAB01"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7487,
          "sourceType": "datasetVersion",
          "datasetId": 4931
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics"
      ],
      "metadata": {
        "id": "j7Xy48114Mit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_regression(name, y_true, y_pred, X_data):\n",
        "\n",
        "    # 1. Basic Metrics (MAE, MSE, RMSE)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 2. R2 and Adjusted R2\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Formula for Adjusted R2\n",
        "    n = len(y_true)\n",
        "    k = X_data.shape[1]\n",
        "    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "    print(f\"--- {name} Performance ---\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "    print(f\"Adjusted R2: {adj_r2:.4f}\")\n",
        "\n",
        "    # 3. Heteroscedasticity Test (Breusch-Pagan)\n",
        "    # Residuals calculation\n",
        "    residuals = y_true - y_pred\n",
        "\n",
        "    # Simple Breusch-Pagan test\n",
        "    print(\"\\n--- Statistical Tests ---\")\n",
        "\n",
        "    bp_test = sms.het_breuschpagan(residuals, sm.add_constant(X_data))\n",
        "    print(f\"Breusch-Pagan p-value: {bp_test[1]:.4f}\")\n",
        "\n",
        "    # 4. Visualizations (Residual Plot & Q-Q Plot)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Residual Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.scatterplot(x=y_pred, y=residuals, alpha=0.5)\n",
        "    plt.axhline(y=0, color='red', linestyle='--')\n",
        "    plt.title(f'Residuals vs Predicted ({name})')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "\n",
        "    # Q-Q Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Normal Q-Q Plot ({name})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-03T16:27:35.513889Z",
          "iopub.execute_input": "2026-01-03T16:27:35.514666Z",
          "iopub.status.idle": "2026-01-03T16:27:35.525119Z",
          "shell.execute_reply.started": "2026-01-03T16:27:35.514628Z",
          "shell.execute_reply": "2026-01-03T16:27:35.524272Z"
        },
        "id": "C-Lqyy4x4Miu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ranked Model Performance (From Best to Worst)\n",
        "1. LightGBM Regressor (The Winner)\n",
        "\n",
        "MAE: 6.7969\n",
        "\n",
        "RMSE: 8.5189\n",
        "\n",
        "R2 Score: 0.2445\n",
        "\n",
        "Comment: The most accurate and efficient model.\n",
        "\n",
        "2. Random Forest Regressor\n",
        "\n",
        "MAE: 6.8011\n",
        "\n",
        "RMSE: 8.5203\n",
        "\n",
        "R2 Score: 0.2411\n",
        "\n",
        "Comment: Highly stable and very close to LightGBM.\n",
        "\n",
        "3. Decision Tree (max_depth=10)\n",
        "\n",
        "MAE: 6.8053\n",
        "\n",
        "RMSE: 8.5370\n",
        "\n",
        "R2 Score: 0.2412\n",
        "\n",
        "Comment: Excellent performance for a single tree model.\n",
        "\n",
        "4. OLS / Ridge / Linear SVR (Tied)\n",
        "\n",
        "MAE: 7.7423\n",
        "\n",
        "RMSE: 9.4044\n",
        "\n",
        "R2 Score: 0.0792\n",
        "\n",
        "Comment: Baseline linear models, showing limited ability to capture complex patterns."
      ],
      "metadata": {
        "id": "BOvaYlkz-fay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Lasso (L1)\n",
        "\n",
        "MAE: 7.7423\n",
        "\n",
        "RMSE: 9.4070\n",
        "\n",
        "R2 Score: 0.0787\n",
        "\n",
        "Comment: Slightly lower performance due to its penalty on features.\n",
        "\n",
        "6. Elastic Net\n",
        "\n",
        "MAE: 7.7455\n",
        "\n",
        "RMSE: 9.4083\n",
        "\n",
        "R2 Score: 0.0785\n",
        "\n",
        "Comment: Similar to Lasso, showing linear limitations.\n",
        "\n",
        "7. KNN (k=11, Manhattan)\n",
        "\n",
        "MAE: 7.8789\n",
        "\n",
        "RMSE: 9.6209\n",
        "\n",
        "R2 Score: NaN\n",
        "\n",
        "Comment: Poor performance and struggled with the data distribution.\n",
        "\n",
        "8. RBF SVR (Sampled)\n",
        "\n",
        "MAE: 7.3980\n",
        "\n",
        "RMSE: 9.8179\n",
        "\n",
        "R2 Score: -0.0002\n",
        "\n",
        "Comment: Worst performance; the negative R2 indicates it performed worse than a horizontal mean line."
      ],
      "metadata": {
        "id": "HlXiEHwGDC0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Verdict:\n",
        "\n",
        "#The Winner is LightGBM Regressor\n",
        "LightGBM is the best performing model for this dataset for the following reasons:\n",
        "\n",
        "Highest Accuracy: It achieved the lowest MAE (6.7969) and the highest R2 Score (0.2445). This means it explains the variance in days_since_prior_order better than any other model.\n",
        "\n",
        "Error Minimization: It has the lowest RMSE (8.5189), indicating that it handles large errors/outliers better than the Decision Tree or Random Forest.\n",
        "\n",
        "Efficiency: Unlike SVR or Random Forest which required \"Sampling,\" LightGBM handled the full 10.6 Million rows efficiently, capturing the global patterns of the entire dataset.\n",
        "\n",
        "Non-Linearity: The jump in R2 score from 0.07 (Linear) to 0.24 (LightGBM) proves that the relationship between user features and order frequency is highly non-linear, which only boosting trees could effectively capture. **bold text**"
      ],
      "metadata": {
        "id": "FOt9PPwmDErX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important Technical Note:\n",
        "All models showed a Breusch-Pagan p-value of 0.0000, which indicates Heteroscedasticity."
      ],
      "metadata": {
        "id": "Oq4WH3IaDUQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Technical Analysis of Model Failures\n",
        "\n",
        "#### 1. Why KNN showed `NaN` for $R^2$ Score?\n",
        "The `NaN` (Not a Number) result for the $R^2$ Score in the KNN models is typically due to:\n",
        "\n",
        "* **Scale and Distance Instability:** KNN relies entirely on calculating distances between points. With a massive dataset (10M+ rows), if the features are not perfectly scaled, the distances can become mathematically unstable.\n",
        "\n",
        "* **Constant Predictions:** If the algorithm fails to find meaningful neighbors due to the data's density and predicts the same value for all instances, the denominator ($SS_{tot}$) in the $R^2$ formula becomes zero, resulting in a mathematical undefined state.\n",
        "* **Computational Overhead:** KNN is not designed for datasets of this magnitude. The **\"curse of dimensionality\"** and the sheer volume of calculations often lead to memory fragmentation or corrupted metric outputs in standard libraries.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 2. Why RBF SVR performed poorly (Negative $R^2$)?\n",
        "The **-0.0002** $R^2$ score indicates that the model is performing worse than a horizontal line representing the mean of the data. This happened because:\n",
        "\n",
        "* **Sampling Bias:** Since RBF SVR is computationally expensive, it was trained on a small sample. It likely **overfitted** to that specific sample and failed to generalize to the patterns in the full dataset.\n",
        "* **Hyperparameter Sensitivity:** The RBF kernel is extremely sensitive to the $C$ (regularization) and $\\gamma$ (gamma) parameters. Without exhaustive Grid Search, the model fails to find the correct \"decision boundary,\" leading to massive errors.\n",
        "* **Noise Sensitivity:** RBF kernels try to map data into higher dimensions. In behavioral data (like order frequency), there is often a lot of \"noise.\" The RBF kernel likely captured the noise instead of the actual trend, making the predictions less accurate than the simple average."
      ],
      "metadata": {
        "id": "gl0dAzb5Dr9A"
      }
    }
  ]
}